{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":14921026,"datasetId":9547343,"databundleVersionId":15787640}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Complete implementation of R-LayerNorm and Bayesian R-LayerNorm for noisy image data WITH PDF REPORT GENERATION\n# Optimized for Kaggle Notebook P100 GPU with 16GB RAM\n# Version: Updated sample sizes for statistical significance","metadata":{}},{"cell_type":"markdown","source":"# Bayesian R-LayerNorm: Robust Normalization for Corrupted Inputs\n\n## Motivation\nStandard normalization layers (BatchNorm, LayerNorm) assume i.i.d. features and can be brittle under input corruptions. This experiment explores two variants — **R-LayerNorm** and **Bayesian R-LayerNorm** — that adapt normalization statistics using local noise estimates. The goal is to see whether these adaptive layers improve robustness on common image corruptions without sacrificing clean accuracy.\n\n## Methods\nWe use a small CNN (three conv layers, Tanh activations) and replace all normalization layers with one of three types:\n- **LayerNorm** (baseline): standard per-channel mean/variance normalization.\n- **R-LayerNorm**: adds a learned scaling of the standard deviation based on local variance (noise estimate).\n- **Bayesian R-LayerNorm**: further refines the scaling using a psi‑function that models uncertainty in the noise estimate, optionally returning an uncertainty map.\n\nWe train on CIFAR‑10 (50k images, 10% held out for validation) and evaluate on four corruptions (Gaussian noise, shot noise, Gaussian blur, contrast) at severity 3.  \nFor each normalization type, we run three independent seeds (42, 123, 456) for 50 epochs with Adam (lr=0.001).  \nTest sets are either generated on‑the‑fly (if CIFAR‑10‑C is unavailable) or loaded from the pre‑generated CIFAR‑10‑C dataset.\n\n## What we compare\n- Training curves (loss, validation accuracy) with mean ± std across seeds.\n- Final test accuracy on each corruption type, again with mean ± std.\n- Improvement over standard LayerNorm to quantify gains.\n- (For Bayesian R‑LayerNorm we also record uncertainty estimates, but they are not analysed in this notebook.)\n\nAll models and results are saved to `/kaggle/working/` for later inspection.","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Cell 1: Setup and imports\n# ============================================================\nimport time\nimport torch\nimport gc\n# Clear CUDA cache aggressively\ntorch.cuda.empty_cache()\ntime.sleep(2)\ngc.collect()\n\n# Reset GPU memory stats\nif torch.cuda.is_available():\n    torch.cuda.reset_peak_memory_stats()\n    torch.cuda.reset_accumulated_memory_stats()\n\n!pip install seaborn tqdm -q\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import CIFAR10\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport warnings\nimport os\nimport random\nfrom PIL import Image, ImageFilter\nimport gc\n\nwarnings.filterwarnings('ignore')\n\n# ============================================================\n# Utility: set seed (called at the start of each run)\n# ============================================================\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Check device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# ============================================================\n# Normalization layer definitions (unchanged)\n# ============================================================\nclass StandardLayerNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1))\n        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n\n    def forward(self, x):\n        mean = x.mean(dim=[2, 3], keepdim=True)\n        var = x.var(dim=[2, 3], keepdim=True, unbiased=False)\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        return self.weight * x_norm + self.bias\n\nclass RLayerNorm(nn.Module):\n    def __init__(self, num_features, lambda_init=0.01, eps=1e-5):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.lambda_param = nn.Parameter(torch.tensor(float(lambda_init)))\n        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1))\n        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        mean = x.mean(dim=[2, 3], keepdim=True)\n        var = x.var(dim=[2, 3], keepdim=True, unbiased=False)\n        std = torch.sqrt(var + self.eps)\n\n        # Local variance for noise estimation\n        x_padded = F.pad(x, (1, 1, 1, 1), mode='reflect')\n        local_means = F.avg_pool2d(x_padded, kernel_size=3, stride=1)\n        local_vars = F.avg_pool2d(x_padded**2, kernel_size=3, stride=1) - local_means**2\n        local_vars = local_vars.clamp(min=0)\n        noise_est = local_vars.mean(dim=[2, 3], keepdim=True)\n\n        lambda_safe = self.lambda_param.clamp(1e-3, 1.0)\n        noise_scale = 1 + lambda_safe * noise_est / (var + self.eps)\n        x_norm = (x - mean) / (std * noise_scale + self.eps)\n        return self.weight * x_norm + self.bias\n\nclass BayesianRLayerNorm(nn.Module):\n    def __init__(self, num_features, lambda_init=0.01, eps=1e-5):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.lambda_param = nn.Parameter(torch.tensor(float(lambda_init)))\n        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1))\n        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n\n    def psi_function(self, t):\n        return torch.log1p(t) - t / (1 + t)\n\n    def forward(self, x, return_uncertainty=False):\n        B, C, H, W = x.shape\n        mean = x.mean(dim=[2, 3], keepdim=True)\n        var = x.var(dim=[2, 3], keepdim=True, unbiased=False)\n        std = torch.sqrt(var + self.eps)\n\n        x_padded = F.pad(x, (1, 1, 1, 1), mode='reflect')\n        local_means = F.avg_pool2d(x_padded, kernel_size=3, stride=1)\n        local_vars = F.avg_pool2d(x_padded**2, kernel_size=3, stride=1) - local_means**2\n        local_vars = local_vars.clamp(min=0)\n        noise_est = local_vars.mean(dim=[2, 3], keepdim=True)\n\n        lambda_safe = self.lambda_param.clamp(1e-3, 1.0)\n        lambdaE = lambda_safe * noise_est / (var + self.eps)\n        psi = self.psi_function(lambdaE)\n\n        effective_std = std * torch.exp(0.5 * psi)\n        normalized = (x - mean) / (effective_std + self.eps)\n        output = self.weight * normalized + self.bias\n\n        if return_uncertainty:\n            uncertainty = 1.0 / (effective_std**2 + self.eps)\n            return output, uncertainty\n        return output\n\n# ============================================================\n# Model definition (same as before)\n# ============================================================\nclass EfficientCNN(nn.Module):\n    def __init__(self, norm_type='layer', num_classes=10):\n        super().__init__()\n        self.norm_type = norm_type\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n        self.norm1 = self._create_norm(16)\n        self.act1 = nn.Tanh()\n        self.pool1 = nn.MaxPool2d(2)\n\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.norm2 = self._create_norm(32)\n        self.act2 = nn.Tanh()\n        self.pool2 = nn.MaxPool2d(2)\n\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.norm3 = self._create_norm(64)\n        self.act3 = nn.Tanh()\n        self.pool3 = nn.MaxPool2d(2)\n\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(64, num_classes)\n\n    def _create_norm(self, num_features):\n        if self.norm_type == 'layer':\n            return StandardLayerNorm(num_features)\n        elif self.norm_type == 'r_layer':\n            return RLayerNorm(num_features)\n        elif self.norm_type == 'bayesian_r_layer':\n            return BayesianRLayerNorm(num_features)\n        else:\n            raise ValueError(f\"Unknown norm_type: {self.norm_type}\")\n\n    def forward(self, x, return_uncertainty=False):\n        x = self.pool1(self.act1(self.norm1(self.conv1(x))))\n        x = self.pool2(self.act2(self.norm2(self.conv2(x))))\n        x = self.conv3(x)\n        if return_uncertainty and isinstance(self.norm3, BayesianRLayerNorm):\n            x, unc = self.norm3(x, return_uncertainty=True)\n            x = self.act3(x)\n            x = self.pool3(x)\n            x = self.global_pool(x).view(x.size(0), -1)\n            return self.fc(x), unc\n        else:\n            x = self.act3(self.norm3(x))\n            x = self.pool3(x)\n            x = self.global_pool(x).view(x.size(0), -1)\n            return self.fc(x)\n\n# ============================================================\n# Dataset with on‑the‑fly corruption (full CIFAR‑10)\n# ============================================================\nclass OnlineNoisyCIFAR10(Dataset):\n    \"\"\"\n    Applies corruption to CIFAR‑10 images on the fly.\n    \"\"\"\n    def __init__(self, root='./data', train=True, noise_type='gaussian', severity=3,\n                 transform=None, download=True):\n        self.clean = CIFAR10(root=root, train=train, download=download, transform=None)\n        self.noise_type = noise_type\n        self.severity = severity\n        # Base transform: to tensor + normalization\n        self.base_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        ])\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.clean)\n\n    def apply_corruption(self, img_pil):\n        img_np = np.array(img_pil).astype(np.float32) / 255.0\n\n        if self.noise_type == 'gaussian':\n            noise = np.random.randn(*img_np.shape) * 0.1 * self.severity\n            img_np = img_np + noise\n        elif self.noise_type == 'shot_noise':\n            mask = np.random.random(img_np.shape) < 0.05 * self.severity\n            salt = np.random.random(mask.sum()) > 0.5\n            img_np_flat = img_np.reshape(-1)\n            mask_flat = mask.reshape(-1)\n            img_np_flat[mask_flat] = salt.astype(np.float32)\n            img_np = img_np_flat.reshape(img_np.shape)\n        elif self.noise_type == 'blur':\n            img_pil = img_pil.filter(ImageFilter.GaussianBlur(radius=self.severity*0.5))\n            img_np = np.array(img_pil).astype(np.float32) / 255.0\n        elif self.noise_type == 'contrast':\n            mean = img_np.mean()\n            contrast_factor = max(0.5, 1.0 - 0.2 * self.severity)\n            img_np = contrast_factor * (img_np - mean) + mean\n\n        img_np = np.clip(img_np, 0, 1)\n        return torch.from_numpy(img_np.transpose(2, 0, 1)).float()\n\n    def __getitem__(self, idx):\n        img_pil, label = self.clean[idx]\n        img_tensor = self.apply_corruption(img_pil)\n        # Apply normalization\n        img_tensor = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))(img_tensor)\n        if self.transform:\n            img_tensor = self.transform(img_tensor)\n        return img_tensor, label\n\n# ============================================================\n# Dataset for CIFAR-10-C (if available)\n# ============================================================\nclass CIFAR10C_Dataset(Dataset):\n    \"\"\"\n    Loads pre‑generated corruptions from CIFAR-10-C.\n    Expects folder structure:\n        /path/to/CIFAR-10-C/\n            gaussian_noise.npy\n            shot_noise.npy\n            gaussian_blur.npy   (maps to 'blur' in our code)\n            contrast.npy\n            labels.npy\n    Each .npy file contains 10000x32x32x3 images, 5 severity levels concatenated.\n    We use severity level 3 (index 2 in 0‑based).\n    \"\"\"\n    def __init__(self, root, corruption_type, severity=3, transform=None):\n        self.root = root\n        self.corruption_type = corruption_type\n        self.severity = severity\n        self.transform = transform\n\n        # Map our names to CIFAR-10-C filenames\n        name_map = {\n            'gaussian': 'gaussian_noise.npy',\n            'shot_noise': 'shot_noise.npy',\n            'blur': 'gaussian_blur.npy',\n            'contrast': 'contrast.npy'\n        }\n        file_name = name_map.get(corruption_type)\n        if file_name is None:\n            raise ValueError(f\"Corruption type {corruption_type} not available in CIFAR-10-C\")\n\n        data_path = os.path.join(root, file_name)\n        labels_path = os.path.join(root, 'labels.npy')\n\n        if not os.path.exists(data_path) or not os.path.exists(labels_path):\n            raise FileNotFoundError(f\"CIFAR-10-C files not found in {root}\")\n\n        # Load images and labels\n        all_images = np.load(data_path)                # (50000, 32, 32, 3) for 5 severities\n        all_labels = np.load(labels_path)              # (10000,) repeated 5 times\n\n        # Select severity level: each severity block is 10000 images\n        start = (severity - 1) * 10000\n        end = severity * 10000\n        self.images = all_images[start:end]            # (10000, 32, 32, 3)\n        self.labels = all_labels[start:end]            # (10000,)\n\n        # Convert to float and scale to [0,1]\n        self.images = self.images.astype(np.float32) / 255.0\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = self.images[idx]                          # (32,32,3) in [0,1]\n        label = self.labels[idx]\n\n        # Convert to tensor (CxHxW)\n        img = torch.from_numpy(img.transpose(2, 0, 1)).float()\n\n        # Normalize with CIFAR-10 stats\n        normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        img = normalize(img)\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n\n# ============================================================\n# Trainer with validation on clean set\n# ============================================================\nclass Trainer:\n    def __init__(self, device, run_seed):\n        self.device = device\n        self.seed = run_seed\n        set_seed(run_seed)\n\n    def clear_memory(self):\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    def train_and_evaluate(self, norm_type, train_loader, val_loader, test_loaders,\n                           epochs=50, lr=0.001):\n        model = EfficientCNN(norm_type=norm_type).to(self.device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n\n        train_losses, val_losses, val_accs = [], [], []\n        best_val_loss = float('inf')\n        best_model_state = None\n\n        for epoch in range(1, epochs+1):\n            # Training\n            model.train()\n            running_loss = 0.0\n            for inputs, targets in train_loader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item()\n            train_loss = running_loss / len(train_loader)\n            train_losses.append(train_loss)\n\n            # Validation on clean data\n            model.eval()\n            val_loss = 0.0\n            correct = 0\n            total = 0\n            with torch.no_grad():\n                for inputs, targets in val_loader:\n                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, targets)\n                    val_loss += loss.item()\n                    _, pred = outputs.max(1)\n                    total += targets.size(0)\n                    correct += pred.eq(targets).sum().item()\n            val_loss /= len(val_loader)\n            val_acc = 100. * correct / total\n            val_losses.append(val_loss)\n            val_accs.append(val_acc)\n\n            print(f\"Epoch {epoch:2d}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n\n            # Save best model based on validation loss\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                best_model_state = model.state_dict().copy()\n\n            if epoch % 10 == 0:\n                self.clear_memory()\n\n        # Load best model for evaluation\n        model.load_state_dict(best_model_state)\n        # Evaluate on each corruption type\n        results = {}\n        for name, loader in test_loaders.items():\n            acc = self.evaluate(model, loader)\n            results[name] = acc\n        return model, results, train_losses, val_losses, val_accs\n\n    def evaluate(self, model, loader):\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, targets in loader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n                outputs = model(inputs)\n                _, pred = outputs.max(1)\n                total += targets.size(0)\n                correct += pred.eq(targets).sum().item()\n        return 100. * correct / total\n\n# ============================================================\n# Prepare data loaders (with optional CIFAR-10-C test set)\n# ============================================================\ndef get_loaders(seed, test_samples_per_noise=10000):\n    set_seed(seed)\n\n    # Paths for Kaggle\n    cifar10_root = '/kaggle/working/data'          # CIFAR-10 will be downloaded here\n    cifar10c_path = '/kaggle/input/cifar-10/CIFAR-10-C'   # Provided by user\n\n    # Clean validation set: 10% of training data\n    full_train = CIFAR10(root=cifar10_root, train=True, download=True, transform=transforms.ToTensor())\n    n_train = len(full_train)\n    indices = list(range(n_train))\n    np.random.shuffle(indices)\n    split = int(0.1 * n_train)\n    val_idx, train_idx = indices[:split], indices[split:]\n\n    # Normalization transform for clean validation\n    norm_transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    clean_transform = transforms.Compose([transforms.ToTensor(), norm_transform])\n\n    # Validation dataset (clean)\n    val_dataset = Subset(CIFAR10(root=cifar10_root, train=True, download=True, transform=clean_transform), val_idx)\n\n    # Training dataset: mix of corruptions generated on the fly\n    noise_types = ['gaussian', 'shot_noise', 'blur', 'contrast']\n    train_datasets = []\n    for nt in noise_types:\n        ds = OnlineNoisyCIFAR10(root=cifar10_root, train=True, noise_type=nt, severity=3, download=True)\n        train_subset = Subset(ds, train_idx)\n        train_datasets.append(train_subset)\n    train_dataset = ConcatDataset(train_datasets)\n\n    # Test loaders: use CIFAR-10-C if available, otherwise fallback to online generation\n    test_loaders = {}\n\n    # Check if CIFAR-10-C exists\n    use_cifar10c = os.path.isdir(cifar10c_path) and all(\n        os.path.exists(os.path.join(cifar10c_path, f)) for f in\n        ['gaussian_noise.npy', 'shot_noise.npy', 'gaussian_blur.npy', 'contrast.npy', 'labels.npy']\n    )\n\n    if use_cifar10c:\n        print(f\"Using CIFAR-10-C from {cifar10c_path}\")\n        for nt in noise_types:\n            # Create dataset for this corruption, severity=3\n            ds = CIFAR10C_Dataset(root=cifar10c_path, corruption_type=nt, severity=3)\n            # Optionally limit samples\n            if test_samples_per_noise and test_samples_per_noise < len(ds):\n                indices = np.random.choice(len(ds), test_samples_per_noise, replace=False)\n                ds = Subset(ds, indices)\n            loader = DataLoader(ds, batch_size=64, shuffle=False, num_workers=2)\n            test_loaders[nt] = loader\n    else:\n        print(\"CIFAR-10-C not found, using on-the-fly corruption for test set.\")\n        for nt in noise_types:\n            test_ds = OnlineNoisyCIFAR10(root=cifar10_root, train=False, noise_type=nt, severity=3, download=True)\n            if test_samples_per_noise and test_samples_per_noise < len(test_ds):\n                test_indices = np.random.choice(len(test_ds), test_samples_per_noise, replace=False)\n                test_ds = Subset(test_ds, test_indices)\n            test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=2)\n            test_loaders[nt] = test_loader\n\n    # DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n    return train_loader, val_loader, test_loaders\n\n# ============================================================\n# Run multiple seeds and collect results\n# ============================================================\ndef run_experiment(seeds=[42, 123, 456], epochs=50, test_samples=10000):\n    norm_types = ['layer', 'r_layer', 'bayesian_r_layer']\n    norm_names = {'layer': 'LayerNorm', 'r_layer': 'R-LayerNorm', 'bayesian_r_layer': 'Bayesian R-LayerNorm'}\n\n    # Create model output directory\n    model_dir = '/kaggle/working/models'\n    os.makedirs(model_dir, exist_ok=True)\n\n    # Store results\n    all_results = {nt: [] for nt in norm_types}\n    all_train_metrics = {nt: [] for nt in norm_types}\n\n    for seed in seeds:\n        print(f\"\\n{'='*60}\\nRUNNING SEED {seed}\\n{'='*60}\")\n        set_seed(seed)\n        trainer = Trainer(device, seed)\n\n        train_loader, val_loader, test_loaders = get_loaders(seed, test_samples_per_noise=test_samples)\n\n        for norm_type in norm_types:\n            print(f\"\\n--- Training {norm_type} ---\")\n            model, results, tr_loss, val_loss, val_acc = trainer.train_and_evaluate(\n                norm_type, train_loader, val_loader, test_loaders, epochs=epochs, lr=0.001)\n\n            # Save model\n            model_path = os.path.join(model_dir, f\"{norm_type}_seed{seed}.pth\")\n            torch.save(model.state_dict(), model_path)\n            print(f\"Model saved to {model_path}\")\n\n            all_results[norm_type].append(results)\n            all_train_metrics[norm_type].append((tr_loss, val_loss, val_acc))\n            trainer.clear_memory()\n\n    # Compute summary statistics\n    summary = {}\n    for norm_type in norm_types:\n        noise_accs = {noise: [] for noise in ['gaussian','shot_noise','blur','contrast']}\n        for seed_results in all_results[norm_type]:\n            for noise, acc in seed_results.items():\n                noise_accs[noise].append(acc)\n        mean_std = {}\n        for noise, acc_list in noise_accs.items():\n            mean = np.mean(acc_list)\n            std = np.std(acc_list)\n            mean_std[noise] = (mean, std)\n        summary[norm_type] = mean_std\n\n    return all_results, all_train_metrics, summary, norm_names\n\n# ============================================================\n# Plotting and saving results (no PDF)\n# ============================================================\ndef generate_plots_and_save_results(all_results, all_train_metrics, summary, norm_names, seeds, epochs):\n    # Output directory\n    out_dir = '/kaggle/working/'\n    viz_dir = os.path.join(out_dir, 'visualizations')\n    os.makedirs(viz_dir, exist_ok=True)\n\n    # Colors and styles\n    colors = {'layer': '#3498db', 'r_layer': '#2ecc71', 'bayesian_r_layer': '#e74c3c'}\n    line_styles = {'layer': '-', 'r_layer': '--', 'bayesian_r_layer': '-.'}\n    noise_types = ['gaussian', 'shot_noise', 'blur', 'contrast']\n    noise_display = [n.replace('_',' ').title() for n in noise_types]\n\n    # 1. Training curves (mean ± std)\n    plt.figure(figsize=(14, 5))\n    plt.subplot(1,2,1)\n    for nt in norm_names.keys():\n        losses = np.array([all_train_metrics[nt][i][0] for i in range(len(seeds))])\n        mean_loss = losses.mean(axis=0)\n        std_loss = losses.std(axis=0)\n        epochs_range = range(1, epochs+1)\n        plt.plot(epochs_range, mean_loss, label=norm_names[nt], color=colors[nt], linestyle=line_styles[nt])\n        plt.fill_between(epochs_range, mean_loss-std_loss, mean_loss+std_loss, color=colors[nt], alpha=0.2)\n    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training Loss (mean ± std)'); plt.legend(); plt.grid(alpha=0.3)\n\n    plt.subplot(1,2,2)\n    for nt in norm_names.keys():\n        val_accs = np.array([all_train_metrics[nt][i][2] for i in range(len(seeds))])\n        mean_acc = val_accs.mean(axis=0)\n        std_acc = val_accs.std(axis=0)\n        plt.plot(epochs_range, mean_acc, label=norm_names[nt], color=colors[nt], linestyle=line_styles[nt])\n        plt.fill_between(epochs_range, mean_acc-std_acc, mean_acc+std_acc, color=colors[nt], alpha=0.2)\n    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.title('Validation Accuracy (mean ± std)'); plt.legend(); plt.grid(alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(os.path.join(viz_dir, 'training_curves.png'), dpi=150)\n    plt.close()\n\n    # 2. Bar chart with error bars\n    plt.figure(figsize=(12,6))\n    x = np.arange(len(noise_types))\n    width = 0.25\n    for i, nt in enumerate(norm_names.keys()):\n        means = [summary[nt][noise][0] for noise in noise_types]\n        stds  = [summary[nt][noise][1] for noise in noise_types]\n        plt.bar(x + i*width - width, means, width, yerr=stds, capsize=3,\n                label=norm_names[nt], color=colors[nt], alpha=0.8)\n    plt.xlabel('Noise Type'); plt.ylabel('Accuracy (%)')\n    plt.title('Test Accuracy on Corruptions (mean ± std)')\n    plt.xticks(x, noise_display, rotation=45, ha='right')\n    plt.legend(); plt.grid(alpha=0.3, axis='y')\n    plt.tight_layout()\n    plt.savefig(os.path.join(viz_dir, 'performance_comparison.png'), dpi=150)\n    plt.close()\n\n    # 3. Improvement over baseline\n    plt.figure(figsize=(10,6))\n    baseline_means = [summary['layer'][noise][0] for noise in noise_types]\n    baseline_stds  = [summary['layer'][noise][1] for noise in noise_types]\n    for nt, label, color in zip(['r_layer','bayesian_r_layer'], ['R-LayerNorm','Bayesian R-LayerNorm'], ['#2ecc71','#e74c3c']):\n        means = [summary[nt][noise][0] for noise in noise_types]\n        stds  = [summary[nt][noise][1] for noise in noise_types]\n        imp = [m - b for m,b in zip(means, baseline_means)]\n        imp_std = [np.sqrt(s**2 + bs**2) for s,bs in zip(stds, baseline_stds)]\n        plt.bar(x + (0 if nt=='r_layer' else width), imp, width,\n                yerr=imp_std, capsize=3, label=label, color=color, alpha=0.8)\n    plt.axhline(0, color='black', linestyle='-', alpha=0.3)\n    plt.xlabel('Noise Type'); plt.ylabel('Accuracy Improvement (%)')\n    plt.title('Improvement Over Standard LayerNorm')\n    plt.xticks(x, noise_display, rotation=45, ha='right')\n    plt.legend(); plt.grid(alpha=0.3, axis='y')\n    plt.tight_layout()\n    plt.savefig(os.path.join(viz_dir, 'improvement_chart.png'), dpi=150)\n    plt.close()\n\n    # 4. Save summary as CSV\n    rows = []\n    for norm_type, nice_name in norm_names.items():\n        for noise in noise_types:\n            mean, std = summary[norm_type][noise]\n            rows.append({\n                'Normalization': nice_name,\n                'Noise Type': noise,\n                'Mean Accuracy': mean,\n                'Std Accuracy': std\n            })\n    df = pd.DataFrame(rows)\n    csv_path = os.path.join(out_dir, 'results_summary.csv')\n    df.to_csv(csv_path, index=False)\n    print(f\"Results saved to {csv_path}\")\n\n    print(f\"\\n✅ All plots saved in {viz_dir}\")\n\n# ============================================================\n# Main execution\n# ============================================================\nif __name__ == \"__main__\":\n    # Parameters\n    SEEDS = [42, 123, 456]          # 3 runs\n    EPOCHS = 50                      # can be increased\n    TEST_SAMPLES = 10000               # per corruption (use 10000 for full test set)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING EXPERIMENT ON KAGGLE\")\n    print(f\"Seeds: {SEEDS}, Epochs: {EPOCHS}, Test samples per corruption: {TEST_SAMPLES}\")\n    print(\"=\"*60)\n\n    all_results, all_train_metrics, summary, norm_names = run_experiment(\n        seeds=SEEDS, epochs=EPOCHS, test_samples=TEST_SAMPLES)\n\n    generate_plots_and_save_results(all_results, all_train_metrics, summary, norm_names, SEEDS, EPOCHS)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"EXPERIMENT COMPLETE. Check /kaggle/working/ for outputs.\")\n    print(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T10:03:49.799045Z","iopub.execute_input":"2026-02-23T10:03:49.800083Z","iopub.status.idle":"2026-02-23T15:19:08.913098Z","shell.execute_reply.started":"2026-02-23T10:03:49.800041Z","shell.execute_reply":"2026-02-23T15:19:08.912455Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla P100-PCIE-16GB\nGPU Memory: 17.1 GB\n\n============================================================\nSTARTING EXPERIMENT ON KAGGLE\nSeeds: [42, 123, 456], Epochs: 50, Test samples per corruption: 10000\n============================================================\n\n============================================================\nRUNNING SEED 42\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:17<00:00, 9.79MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using CIFAR-10-C from /kaggle/input/cifar-10/CIFAR-10-C\n\n--- Training layer ---\nEpoch  1/50 | Train Loss: 1.7386 | Val Loss: 1.4678 | Val Acc: 47.28%\nEpoch  2/50 | Train Loss: 1.4443 | Val Loss: 1.3541 | Val Acc: 50.90%\nEpoch  3/50 | Train Loss: 1.3487 | Val Loss: 1.2699 | Val Acc: 54.26%\nEpoch  4/50 | Train Loss: 1.2917 | Val Loss: 1.2131 | Val Acc: 57.24%\nEpoch  5/50 | Train Loss: 1.2532 | Val Loss: 1.1716 | Val Acc: 58.02%\nEpoch  6/50 | Train Loss: 1.2207 | Val Loss: 1.1611 | Val Acc: 58.68%\nEpoch  7/50 | Train Loss: 1.1954 | Val Loss: 1.1319 | Val Acc: 59.60%\nEpoch  8/50 | Train Loss: 1.1763 | Val Loss: 1.1219 | Val Acc: 59.90%\nEpoch  9/50 | Train Loss: 1.1575 | Val Loss: 1.1225 | Val Acc: 60.20%\nEpoch 10/50 | Train Loss: 1.1421 | Val Loss: 1.1115 | Val Acc: 60.56%\nEpoch 11/50 | Train Loss: 1.1308 | Val Loss: 1.0960 | Val Acc: 61.34%\nEpoch 12/50 | Train Loss: 1.1178 | Val Loss: 1.0865 | Val Acc: 61.70%\nEpoch 13/50 | Train Loss: 1.1074 | Val Loss: 1.0889 | Val Acc: 61.74%\nEpoch 14/50 | Train Loss: 1.0981 | Val Loss: 1.0841 | Val Acc: 61.58%\nEpoch 15/50 | Train Loss: 1.0905 | Val Loss: 1.0725 | Val Acc: 62.26%\nEpoch 16/50 | Train Loss: 1.0840 | Val Loss: 1.0643 | Val Acc: 62.86%\nEpoch 17/50 | Train Loss: 1.0756 | Val Loss: 1.0588 | Val Acc: 62.68%\nEpoch 18/50 | Train Loss: 1.0692 | Val Loss: 1.0577 | Val Acc: 62.94%\nEpoch 19/50 | Train Loss: 1.0608 | Val Loss: 1.0679 | Val Acc: 62.78%\nEpoch 20/50 | Train Loss: 1.0566 | Val Loss: 1.0709 | Val Acc: 61.98%\nEpoch 21/50 | Train Loss: 1.0499 | Val Loss: 1.0772 | Val Acc: 61.52%\nEpoch 22/50 | Train Loss: 1.0447 | Val Loss: 1.0426 | Val Acc: 63.06%\nEpoch 23/50 | Train Loss: 1.0423 | Val Loss: 1.0554 | Val Acc: 62.90%\nEpoch 24/50 | Train Loss: 1.0356 | Val Loss: 1.0408 | Val Acc: 63.10%\nEpoch 25/50 | Train Loss: 1.0311 | Val Loss: 1.0569 | Val Acc: 62.70%\nEpoch 26/50 | Train Loss: 1.0274 | Val Loss: 1.0321 | Val Acc: 63.78%\nEpoch 27/50 | Train Loss: 1.0257 | Val Loss: 1.0459 | Val Acc: 62.90%\nEpoch 28/50 | Train Loss: 1.0224 | Val Loss: 1.0521 | Val Acc: 63.50%\nEpoch 29/50 | Train Loss: 1.0208 | Val Loss: 1.0461 | Val Acc: 63.72%\nEpoch 30/50 | Train Loss: 1.0155 | Val Loss: 1.0763 | Val Acc: 62.48%\nEpoch 31/50 | Train Loss: 1.0133 | Val Loss: 1.0355 | Val Acc: 63.42%\nEpoch 32/50 | Train Loss: 1.0104 | Val Loss: 1.0411 | Val Acc: 63.24%\nEpoch 33/50 | Train Loss: 1.0078 | Val Loss: 1.0311 | Val Acc: 64.30%\nEpoch 34/50 | Train Loss: 1.0041 | Val Loss: 1.0345 | Val Acc: 63.60%\nEpoch 35/50 | Train Loss: 1.0019 | Val Loss: 1.0374 | Val Acc: 63.32%\nEpoch 36/50 | Train Loss: 1.0002 | Val Loss: 1.0402 | Val Acc: 63.56%\nEpoch 37/50 | Train Loss: 0.9977 | Val Loss: 1.0311 | Val Acc: 64.04%\nEpoch 38/50 | Train Loss: 0.9962 | Val Loss: 1.0257 | Val Acc: 63.24%\nEpoch 39/50 | Train Loss: 0.9925 | Val Loss: 1.0245 | Val Acc: 64.00%\nEpoch 40/50 | Train Loss: 0.9904 | Val Loss: 1.0326 | Val Acc: 64.10%\nEpoch 41/50 | Train Loss: 0.9887 | Val Loss: 1.0328 | Val Acc: 64.04%\nEpoch 42/50 | Train Loss: 0.9865 | Val Loss: 1.0348 | Val Acc: 63.62%\nEpoch 43/50 | Train Loss: 0.9833 | Val Loss: 1.0278 | Val Acc: 64.12%\nEpoch 44/50 | Train Loss: 0.9818 | Val Loss: 1.0247 | Val Acc: 64.10%\nEpoch 45/50 | Train Loss: 0.9823 | Val Loss: 1.0235 | Val Acc: 64.08%\nEpoch 46/50 | Train Loss: 0.9766 | Val Loss: 1.0510 | Val Acc: 63.32%\nEpoch 47/50 | Train Loss: 0.9760 | Val Loss: 1.0179 | Val Acc: 64.14%\nEpoch 48/50 | Train Loss: 0.9750 | Val Loss: 1.0280 | Val Acc: 64.48%\nEpoch 49/50 | Train Loss: 0.9736 | Val Loss: 1.0407 | Val Acc: 63.90%\nEpoch 50/50 | Train Loss: 0.9730 | Val Loss: 1.0365 | Val Acc: 64.26%\nModel saved to /kaggle/working/models/layer_seed42.pth\n\n--- Training r_layer ---\nEpoch  1/50 | Train Loss: 1.7302 | Val Loss: 1.4376 | Val Acc: 48.06%\nEpoch  2/50 | Train Loss: 1.4143 | Val Loss: 1.3212 | Val Acc: 52.40%\nEpoch  3/50 | Train Loss: 1.3227 | Val Loss: 1.2132 | Val Acc: 56.92%\nEpoch  4/50 | Train Loss: 1.2673 | Val Loss: 1.1938 | Val Acc: 57.32%\nEpoch  5/50 | Train Loss: 1.2313 | Val Loss: 1.1466 | Val Acc: 59.12%\nEpoch  6/50 | Train Loss: 1.2029 | Val Loss: 1.1378 | Val Acc: 59.04%\nEpoch  7/50 | Train Loss: 1.1819 | Val Loss: 1.1068 | Val Acc: 60.42%\nEpoch  8/50 | Train Loss: 1.1634 | Val Loss: 1.0884 | Val Acc: 61.20%\nEpoch  9/50 | Train Loss: 1.1470 | Val Loss: 1.1163 | Val Acc: 60.30%\nEpoch 10/50 | Train Loss: 1.1326 | Val Loss: 1.0732 | Val Acc: 61.72%\nEpoch 11/50 | Train Loss: 1.1212 | Val Loss: 1.0481 | Val Acc: 62.62%\nEpoch 12/50 | Train Loss: 1.1107 | Val Loss: 1.0657 | Val Acc: 62.12%\nEpoch 13/50 | Train Loss: 1.1007 | Val Loss: 1.0422 | Val Acc: 62.58%\nEpoch 14/50 | Train Loss: 1.0919 | Val Loss: 1.0422 | Val Acc: 62.42%\nEpoch 15/50 | Train Loss: 1.0834 | Val Loss: 1.0458 | Val Acc: 63.20%\nEpoch 16/50 | Train Loss: 1.0788 | Val Loss: 1.0407 | Val Acc: 62.66%\nEpoch 17/50 | Train Loss: 1.0736 | Val Loss: 1.0507 | Val Acc: 62.74%\nEpoch 18/50 | Train Loss: 1.0647 | Val Loss: 1.0275 | Val Acc: 63.58%\nEpoch 19/50 | Train Loss: 1.0598 | Val Loss: 1.0283 | Val Acc: 63.74%\nEpoch 20/50 | Train Loss: 1.0540 | Val Loss: 1.0193 | Val Acc: 64.00%\nEpoch 21/50 | Train Loss: 1.0475 | Val Loss: 1.0269 | Val Acc: 63.40%\nEpoch 22/50 | Train Loss: 1.0459 | Val Loss: 1.0222 | Val Acc: 63.86%\nEpoch 23/50 | Train Loss: 1.0410 | Val Loss: 1.0277 | Val Acc: 64.00%\nEpoch 24/50 | Train Loss: 1.0348 | Val Loss: 1.0195 | Val Acc: 64.22%\nEpoch 25/50 | Train Loss: 1.0328 | Val Loss: 1.0123 | Val Acc: 64.34%\nEpoch 26/50 | Train Loss: 1.0290 | Val Loss: 1.0132 | Val Acc: 64.06%\nEpoch 27/50 | Train Loss: 1.0273 | Val Loss: 1.0072 | Val Acc: 64.98%\nEpoch 28/50 | Train Loss: 1.0204 | Val Loss: 1.0258 | Val Acc: 64.00%\nEpoch 29/50 | Train Loss: 1.0194 | Val Loss: 1.0110 | Val Acc: 64.84%\nEpoch 30/50 | Train Loss: 1.0169 | Val Loss: 1.0079 | Val Acc: 64.78%\nEpoch 31/50 | Train Loss: 1.0130 | Val Loss: 0.9995 | Val Acc: 64.96%\nEpoch 32/50 | Train Loss: 1.0097 | Val Loss: 1.0131 | Val Acc: 63.44%\nEpoch 33/50 | Train Loss: 1.0050 | Val Loss: 1.0004 | Val Acc: 64.90%\nEpoch 34/50 | Train Loss: 1.0041 | Val Loss: 1.0121 | Val Acc: 64.44%\nEpoch 35/50 | Train Loss: 1.0008 | Val Loss: 1.0020 | Val Acc: 64.94%\nEpoch 36/50 | Train Loss: 0.9999 | Val Loss: 1.0232 | Val Acc: 63.98%\nEpoch 37/50 | Train Loss: 0.9962 | Val Loss: 1.0013 | Val Acc: 64.76%\nEpoch 38/50 | Train Loss: 0.9944 | Val Loss: 0.9899 | Val Acc: 65.50%\nEpoch 39/50 | Train Loss: 0.9950 | Val Loss: 1.0099 | Val Acc: 64.80%\nEpoch 40/50 | Train Loss: 0.9905 | Val Loss: 1.0169 | Val Acc: 64.04%\nEpoch 41/50 | Train Loss: 0.9885 | Val Loss: 1.0070 | Val Acc: 64.36%\nEpoch 42/50 | Train Loss: 0.9864 | Val Loss: 1.0050 | Val Acc: 65.22%\nEpoch 43/50 | Train Loss: 0.9849 | Val Loss: 1.0057 | Val Acc: 64.72%\nEpoch 44/50 | Train Loss: 0.9841 | Val Loss: 1.0232 | Val Acc: 64.26%\nEpoch 45/50 | Train Loss: 0.9800 | Val Loss: 0.9989 | Val Acc: 65.06%\nEpoch 46/50 | Train Loss: 0.9799 | Val Loss: 0.9895 | Val Acc: 65.46%\nEpoch 47/50 | Train Loss: 0.9783 | Val Loss: 0.9915 | Val Acc: 65.52%\nEpoch 48/50 | Train Loss: 0.9758 | Val Loss: 0.9891 | Val Acc: 65.48%\nEpoch 49/50 | Train Loss: 0.9738 | Val Loss: 1.0096 | Val Acc: 64.84%\nEpoch 50/50 | Train Loss: 0.9744 | Val Loss: 0.9916 | Val Acc: 65.20%\nModel saved to /kaggle/working/models/r_layer_seed42.pth\n\n--- Training bayesian_r_layer ---\nEpoch  1/50 | Train Loss: 1.7389 | Val Loss: 1.4521 | Val Acc: 47.76%\nEpoch  2/50 | Train Loss: 1.4323 | Val Loss: 1.3094 | Val Acc: 53.84%\nEpoch  3/50 | Train Loss: 1.3312 | Val Loss: 1.2266 | Val Acc: 56.12%\nEpoch  4/50 | Train Loss: 1.2713 | Val Loss: 1.1779 | Val Acc: 57.40%\nEpoch  5/50 | Train Loss: 1.2341 | Val Loss: 1.1551 | Val Acc: 57.42%\nEpoch  6/50 | Train Loss: 1.2041 | Val Loss: 1.1629 | Val Acc: 57.46%\nEpoch  7/50 | Train Loss: 1.1835 | Val Loss: 1.1163 | Val Acc: 59.74%\nEpoch  8/50 | Train Loss: 1.1642 | Val Loss: 1.1153 | Val Acc: 59.48%\nEpoch  9/50 | Train Loss: 1.1482 | Val Loss: 1.1076 | Val Acc: 60.44%\nEpoch 10/50 | Train Loss: 1.1320 | Val Loss: 1.0681 | Val Acc: 61.90%\nEpoch 11/50 | Train Loss: 1.1203 | Val Loss: 1.0685 | Val Acc: 61.94%\nEpoch 12/50 | Train Loss: 1.1081 | Val Loss: 1.0608 | Val Acc: 61.72%\nEpoch 13/50 | Train Loss: 1.0992 | Val Loss: 1.0424 | Val Acc: 63.18%\nEpoch 14/50 | Train Loss: 1.0901 | Val Loss: 1.0436 | Val Acc: 62.72%\nEpoch 15/50 | Train Loss: 1.0814 | Val Loss: 1.0492 | Val Acc: 62.44%\nEpoch 16/50 | Train Loss: 1.0750 | Val Loss: 1.0377 | Val Acc: 63.06%\nEpoch 17/50 | Train Loss: 1.0690 | Val Loss: 1.0396 | Val Acc: 62.82%\nEpoch 18/50 | Train Loss: 1.0630 | Val Loss: 1.0396 | Val Acc: 62.56%\nEpoch 19/50 | Train Loss: 1.0558 | Val Loss: 1.0331 | Val Acc: 63.16%\nEpoch 20/50 | Train Loss: 1.0493 | Val Loss: 1.0257 | Val Acc: 63.32%\nEpoch 21/50 | Train Loss: 1.0461 | Val Loss: 1.0169 | Val Acc: 64.04%\nEpoch 22/50 | Train Loss: 1.0415 | Val Loss: 1.0375 | Val Acc: 63.06%\nEpoch 23/50 | Train Loss: 1.0373 | Val Loss: 1.0246 | Val Acc: 63.24%\nEpoch 24/50 | Train Loss: 1.0333 | Val Loss: 1.0259 | Val Acc: 63.60%\nEpoch 25/50 | Train Loss: 1.0276 | Val Loss: 1.0197 | Val Acc: 63.74%\nEpoch 26/50 | Train Loss: 1.0263 | Val Loss: 1.0409 | Val Acc: 62.62%\nEpoch 27/50 | Train Loss: 1.0240 | Val Loss: 1.0182 | Val Acc: 63.84%\nEpoch 28/50 | Train Loss: 1.0176 | Val Loss: 1.0175 | Val Acc: 63.44%\nEpoch 29/50 | Train Loss: 1.0128 | Val Loss: 1.0185 | Val Acc: 64.04%\nEpoch 30/50 | Train Loss: 1.0132 | Val Loss: 1.0355 | Val Acc: 63.28%\nEpoch 31/50 | Train Loss: 1.0076 | Val Loss: 1.0133 | Val Acc: 64.14%\nEpoch 32/50 | Train Loss: 1.0043 | Val Loss: 0.9976 | Val Acc: 64.54%\nEpoch 33/50 | Train Loss: 1.0039 | Val Loss: 1.0259 | Val Acc: 63.42%\nEpoch 34/50 | Train Loss: 1.0007 | Val Loss: 1.0072 | Val Acc: 64.50%\nEpoch 35/50 | Train Loss: 0.9990 | Val Loss: 1.0085 | Val Acc: 63.90%\nEpoch 36/50 | Train Loss: 0.9937 | Val Loss: 0.9975 | Val Acc: 64.28%\nEpoch 37/50 | Train Loss: 0.9927 | Val Loss: 1.0037 | Val Acc: 64.66%\nEpoch 38/50 | Train Loss: 0.9898 | Val Loss: 1.0048 | Val Acc: 64.80%\nEpoch 39/50 | Train Loss: 0.9878 | Val Loss: 1.0223 | Val Acc: 64.14%\nEpoch 40/50 | Train Loss: 0.9830 | Val Loss: 0.9966 | Val Acc: 64.64%\nEpoch 41/50 | Train Loss: 0.9843 | Val Loss: 1.0257 | Val Acc: 63.82%\nEpoch 42/50 | Train Loss: 0.9827 | Val Loss: 1.0189 | Val Acc: 64.18%\nEpoch 43/50 | Train Loss: 0.9795 | Val Loss: 1.0164 | Val Acc: 64.38%\nEpoch 44/50 | Train Loss: 0.9761 | Val Loss: 1.0027 | Val Acc: 64.68%\nEpoch 45/50 | Train Loss: 0.9742 | Val Loss: 0.9968 | Val Acc: 64.40%\nEpoch 46/50 | Train Loss: 0.9749 | Val Loss: 1.0026 | Val Acc: 64.48%\nEpoch 47/50 | Train Loss: 0.9691 | Val Loss: 0.9942 | Val Acc: 64.94%\nEpoch 48/50 | Train Loss: 0.9707 | Val Loss: 0.9942 | Val Acc: 64.74%\nEpoch 49/50 | Train Loss: 0.9686 | Val Loss: 0.9968 | Val Acc: 64.70%\nEpoch 50/50 | Train Loss: 0.9646 | Val Loss: 0.9892 | Val Acc: 64.94%\nModel saved to /kaggle/working/models/bayesian_r_layer_seed42.pth\n\n============================================================\nRUNNING SEED 123\n============================================================\nUsing CIFAR-10-C from /kaggle/input/cifar-10/CIFAR-10-C\n\n--- Training layer ---\nEpoch  1/50 | Train Loss: 1.7350 | Val Loss: 1.4537 | Val Acc: 48.12%\nEpoch  2/50 | Train Loss: 1.4288 | Val Loss: 1.3208 | Val Acc: 53.24%\nEpoch  3/50 | Train Loss: 1.3323 | Val Loss: 1.2325 | Val Acc: 56.50%\nEpoch  4/50 | Train Loss: 1.2729 | Val Loss: 1.1903 | Val Acc: 57.44%\nEpoch  5/50 | Train Loss: 1.2347 | Val Loss: 1.1686 | Val Acc: 58.76%\nEpoch  6/50 | Train Loss: 1.2049 | Val Loss: 1.1542 | Val Acc: 59.16%\nEpoch  7/50 | Train Loss: 1.1801 | Val Loss: 1.1292 | Val Acc: 59.98%\nEpoch  8/50 | Train Loss: 1.1591 | Val Loss: 1.1295 | Val Acc: 60.72%\nEpoch  9/50 | Train Loss: 1.1446 | Val Loss: 1.0997 | Val Acc: 61.10%\nEpoch 10/50 | Train Loss: 1.1303 | Val Loss: 1.1044 | Val Acc: 61.32%\nEpoch 11/50 | Train Loss: 1.1178 | Val Loss: 1.0887 | Val Acc: 61.44%\nEpoch 12/50 | Train Loss: 1.1059 | Val Loss: 1.0954 | Val Acc: 61.56%\nEpoch 13/50 | Train Loss: 1.0955 | Val Loss: 1.0827 | Val Acc: 61.54%\nEpoch 14/50 | Train Loss: 1.0879 | Val Loss: 1.0765 | Val Acc: 61.84%\nEpoch 15/50 | Train Loss: 1.0791 | Val Loss: 1.0739 | Val Acc: 62.54%\nEpoch 16/50 | Train Loss: 1.0720 | Val Loss: 1.0846 | Val Acc: 61.76%\nEpoch 17/50 | Train Loss: 1.0648 | Val Loss: 1.0689 | Val Acc: 62.52%\nEpoch 18/50 | Train Loss: 1.0587 | Val Loss: 1.0688 | Val Acc: 62.20%\nEpoch 19/50 | Train Loss: 1.0540 | Val Loss: 1.0585 | Val Acc: 62.82%\nEpoch 20/50 | Train Loss: 1.0468 | Val Loss: 1.0654 | Val Acc: 63.40%\nEpoch 21/50 | Train Loss: 1.0417 | Val Loss: 1.0481 | Val Acc: 63.76%\nEpoch 22/50 | Train Loss: 1.0357 | Val Loss: 1.0593 | Val Acc: 62.88%\nEpoch 23/50 | Train Loss: 1.0336 | Val Loss: 1.0536 | Val Acc: 63.70%\nEpoch 24/50 | Train Loss: 1.0285 | Val Loss: 1.0563 | Val Acc: 63.20%\nEpoch 25/50 | Train Loss: 1.0248 | Val Loss: 1.0504 | Val Acc: 63.16%\nEpoch 26/50 | Train Loss: 1.0195 | Val Loss: 1.0452 | Val Acc: 64.12%\nEpoch 27/50 | Train Loss: 1.0183 | Val Loss: 1.0473 | Val Acc: 63.98%\nEpoch 28/50 | Train Loss: 1.0144 | Val Loss: 1.0406 | Val Acc: 63.66%\nEpoch 29/50 | Train Loss: 1.0105 | Val Loss: 1.0504 | Val Acc: 63.48%\nEpoch 30/50 | Train Loss: 1.0074 | Val Loss: 1.0484 | Val Acc: 64.44%\nEpoch 31/50 | Train Loss: 1.0045 | Val Loss: 1.0365 | Val Acc: 64.38%\nEpoch 32/50 | Train Loss: 1.0033 | Val Loss: 1.0381 | Val Acc: 64.10%\nEpoch 33/50 | Train Loss: 1.0008 | Val Loss: 1.0563 | Val Acc: 64.04%\nEpoch 34/50 | Train Loss: 0.9968 | Val Loss: 1.0376 | Val Acc: 64.08%\nEpoch 35/50 | Train Loss: 0.9942 | Val Loss: 1.0378 | Val Acc: 64.52%\nEpoch 36/50 | Train Loss: 0.9921 | Val Loss: 1.0402 | Val Acc: 64.40%\nEpoch 37/50 | Train Loss: 0.9904 | Val Loss: 1.0473 | Val Acc: 63.92%\nEpoch 38/50 | Train Loss: 0.9870 | Val Loss: 1.0275 | Val Acc: 65.16%\nEpoch 39/50 | Train Loss: 0.9860 | Val Loss: 1.0300 | Val Acc: 64.78%\nEpoch 40/50 | Train Loss: 0.9843 | Val Loss: 1.0419 | Val Acc: 64.16%\nEpoch 41/50 | Train Loss: 0.9815 | Val Loss: 1.0221 | Val Acc: 64.76%\nEpoch 42/50 | Train Loss: 0.9820 | Val Loss: 1.0287 | Val Acc: 64.34%\nEpoch 43/50 | Train Loss: 0.9779 | Val Loss: 1.0362 | Val Acc: 64.64%\nEpoch 44/50 | Train Loss: 0.9768 | Val Loss: 1.0381 | Val Acc: 64.18%\nEpoch 45/50 | Train Loss: 0.9751 | Val Loss: 1.0233 | Val Acc: 64.60%\nEpoch 46/50 | Train Loss: 0.9739 | Val Loss: 1.0289 | Val Acc: 64.42%\nEpoch 47/50 | Train Loss: 0.9723 | Val Loss: 1.0282 | Val Acc: 64.56%\nEpoch 48/50 | Train Loss: 0.9720 | Val Loss: 1.0224 | Val Acc: 64.76%\nEpoch 49/50 | Train Loss: 0.9672 | Val Loss: 1.0224 | Val Acc: 64.78%\nEpoch 50/50 | Train Loss: 0.9666 | Val Loss: 1.0209 | Val Acc: 64.56%\nModel saved to /kaggle/working/models/layer_seed123.pth\n\n--- Training r_layer ---\nEpoch  1/50 | Train Loss: 1.7223 | Val Loss: 1.4580 | Val Acc: 48.22%\nEpoch  2/50 | Train Loss: 1.4225 | Val Loss: 1.3185 | Val Acc: 52.66%\nEpoch  3/50 | Train Loss: 1.3292 | Val Loss: 1.2413 | Val Acc: 55.48%\nEpoch  4/50 | Train Loss: 1.2752 | Val Loss: 1.2098 | Val Acc: 57.38%\nEpoch  5/50 | Train Loss: 1.2385 | Val Loss: 1.1866 | Val Acc: 57.20%\nEpoch  6/50 | Train Loss: 1.2105 | Val Loss: 1.1654 | Val Acc: 58.02%\nEpoch  7/50 | Train Loss: 1.1884 | Val Loss: 1.1284 | Val Acc: 58.92%\nEpoch  8/50 | Train Loss: 1.1695 | Val Loss: 1.1243 | Val Acc: 59.96%\nEpoch  9/50 | Train Loss: 1.1523 | Val Loss: 1.1073 | Val Acc: 59.94%\nEpoch 10/50 | Train Loss: 1.1389 | Val Loss: 1.1105 | Val Acc: 60.64%\nEpoch 11/50 | Train Loss: 1.1276 | Val Loss: 1.1128 | Val Acc: 60.44%\nEpoch 12/50 | Train Loss: 1.1173 | Val Loss: 1.1048 | Val Acc: 60.48%\nEpoch 13/50 | Train Loss: 1.1048 | Val Loss: 1.1108 | Val Acc: 60.26%\nEpoch 14/50 | Train Loss: 1.0988 | Val Loss: 1.0868 | Val Acc: 61.64%\nEpoch 15/50 | Train Loss: 1.0884 | Val Loss: 1.0863 | Val Acc: 61.48%\nEpoch 16/50 | Train Loss: 1.0825 | Val Loss: 1.0780 | Val Acc: 61.96%\nEpoch 17/50 | Train Loss: 1.0768 | Val Loss: 1.0791 | Val Acc: 61.84%\nEpoch 18/50 | Train Loss: 1.0721 | Val Loss: 1.0634 | Val Acc: 62.18%\nEpoch 19/50 | Train Loss: 1.0661 | Val Loss: 1.0761 | Val Acc: 62.48%\nEpoch 20/50 | Train Loss: 1.0604 | Val Loss: 1.0731 | Val Acc: 61.50%\nEpoch 21/50 | Train Loss: 1.0557 | Val Loss: 1.0617 | Val Acc: 62.54%\nEpoch 22/50 | Train Loss: 1.0475 | Val Loss: 1.0595 | Val Acc: 62.36%\nEpoch 23/50 | Train Loss: 1.0470 | Val Loss: 1.0558 | Val Acc: 62.78%\nEpoch 24/50 | Train Loss: 1.0397 | Val Loss: 1.0637 | Val Acc: 62.44%\nEpoch 25/50 | Train Loss: 1.0368 | Val Loss: 1.0551 | Val Acc: 62.58%\nEpoch 26/50 | Train Loss: 1.0329 | Val Loss: 1.0553 | Val Acc: 63.38%\nEpoch 27/50 | Train Loss: 1.0298 | Val Loss: 1.0467 | Val Acc: 63.26%\nEpoch 28/50 | Train Loss: 1.0275 | Val Loss: 1.0643 | Val Acc: 62.56%\nEpoch 29/50 | Train Loss: 1.0242 | Val Loss: 1.0623 | Val Acc: 63.00%\nEpoch 30/50 | Train Loss: 1.0209 | Val Loss: 1.0495 | Val Acc: 62.98%\nEpoch 31/50 | Train Loss: 1.0174 | Val Loss: 1.0534 | Val Acc: 63.12%\nEpoch 32/50 | Train Loss: 1.0117 | Val Loss: 1.0421 | Val Acc: 63.20%\nEpoch 33/50 | Train Loss: 1.0118 | Val Loss: 1.0581 | Val Acc: 62.92%\nEpoch 34/50 | Train Loss: 1.0073 | Val Loss: 1.0544 | Val Acc: 63.54%\nEpoch 35/50 | Train Loss: 1.0038 | Val Loss: 1.0537 | Val Acc: 62.56%\nEpoch 36/50 | Train Loss: 1.0032 | Val Loss: 1.0503 | Val Acc: 63.42%\nEpoch 37/50 | Train Loss: 0.9986 | Val Loss: 1.0413 | Val Acc: 63.32%\nEpoch 38/50 | Train Loss: 1.0004 | Val Loss: 1.0739 | Val Acc: 62.86%\nEpoch 39/50 | Train Loss: 0.9968 | Val Loss: 1.0479 | Val Acc: 63.16%\nEpoch 40/50 | Train Loss: 0.9957 | Val Loss: 1.0568 | Val Acc: 62.86%\nEpoch 41/50 | Train Loss: 0.9945 | Val Loss: 1.0531 | Val Acc: 62.96%\nEpoch 42/50 | Train Loss: 0.9911 | Val Loss: 1.0579 | Val Acc: 63.00%\nEpoch 43/50 | Train Loss: 0.9879 | Val Loss: 1.0481 | Val Acc: 63.30%\nEpoch 44/50 | Train Loss: 0.9870 | Val Loss: 1.0467 | Val Acc: 63.30%\nEpoch 45/50 | Train Loss: 0.9839 | Val Loss: 1.0660 | Val Acc: 63.32%\nEpoch 46/50 | Train Loss: 0.9830 | Val Loss: 1.0590 | Val Acc: 62.94%\nEpoch 47/50 | Train Loss: 0.9800 | Val Loss: 1.0553 | Val Acc: 62.90%\nEpoch 48/50 | Train Loss: 0.9784 | Val Loss: 1.0434 | Val Acc: 63.46%\nEpoch 49/50 | Train Loss: 0.9750 | Val Loss: 1.0426 | Val Acc: 63.40%\nEpoch 50/50 | Train Loss: 0.9761 | Val Loss: 1.0683 | Val Acc: 62.52%\nModel saved to /kaggle/working/models/r_layer_seed123.pth\n\n--- Training bayesian_r_layer ---\nEpoch  1/50 | Train Loss: 1.7415 | Val Loss: 1.4447 | Val Acc: 48.50%\nEpoch  2/50 | Train Loss: 1.4238 | Val Loss: 1.2908 | Val Acc: 54.38%\nEpoch  3/50 | Train Loss: 1.3261 | Val Loss: 1.2498 | Val Acc: 55.58%\nEpoch  4/50 | Train Loss: 1.2688 | Val Loss: 1.1974 | Val Acc: 57.04%\nEpoch  5/50 | Train Loss: 1.2306 | Val Loss: 1.1901 | Val Acc: 57.40%\nEpoch  6/50 | Train Loss: 1.1992 | Val Loss: 1.1432 | Val Acc: 59.18%\nEpoch  7/50 | Train Loss: 1.1776 | Val Loss: 1.1303 | Val Acc: 60.16%\nEpoch  8/50 | Train Loss: 1.1598 | Val Loss: 1.1240 | Val Acc: 60.48%\nEpoch  9/50 | Train Loss: 1.1440 | Val Loss: 1.1060 | Val Acc: 61.58%\nEpoch 10/50 | Train Loss: 1.1281 | Val Loss: 1.1077 | Val Acc: 60.62%\nEpoch 11/50 | Train Loss: 1.1170 | Val Loss: 1.1176 | Val Acc: 60.44%\nEpoch 12/50 | Train Loss: 1.1066 | Val Loss: 1.0753 | Val Acc: 62.80%\nEpoch 13/50 | Train Loss: 1.0962 | Val Loss: 1.0862 | Val Acc: 61.84%\nEpoch 14/50 | Train Loss: 1.0882 | Val Loss: 1.0884 | Val Acc: 61.70%\nEpoch 15/50 | Train Loss: 1.0802 | Val Loss: 1.0875 | Val Acc: 61.60%\nEpoch 16/50 | Train Loss: 1.0734 | Val Loss: 1.0812 | Val Acc: 62.60%\nEpoch 17/50 | Train Loss: 1.0666 | Val Loss: 1.0686 | Val Acc: 62.64%\nEpoch 18/50 | Train Loss: 1.0599 | Val Loss: 1.0735 | Val Acc: 62.48%\nEpoch 19/50 | Train Loss: 1.0558 | Val Loss: 1.0677 | Val Acc: 62.96%\nEpoch 20/50 | Train Loss: 1.0484 | Val Loss: 1.0932 | Val Acc: 62.10%\nEpoch 21/50 | Train Loss: 1.0431 | Val Loss: 1.0816 | Val Acc: 62.22%\nEpoch 22/50 | Train Loss: 1.0397 | Val Loss: 1.0569 | Val Acc: 62.90%\nEpoch 23/50 | Train Loss: 1.0360 | Val Loss: 1.0606 | Val Acc: 63.12%\nEpoch 24/50 | Train Loss: 1.0304 | Val Loss: 1.0625 | Val Acc: 63.10%\nEpoch 25/50 | Train Loss: 1.0271 | Val Loss: 1.0563 | Val Acc: 63.56%\nEpoch 26/50 | Train Loss: 1.0224 | Val Loss: 1.0650 | Val Acc: 63.14%\nEpoch 27/50 | Train Loss: 1.0188 | Val Loss: 1.0690 | Val Acc: 62.96%\nEpoch 28/50 | Train Loss: 1.0148 | Val Loss: 1.0716 | Val Acc: 62.40%\nEpoch 29/50 | Train Loss: 1.0123 | Val Loss: 1.0558 | Val Acc: 63.46%\nEpoch 30/50 | Train Loss: 1.0108 | Val Loss: 1.0752 | Val Acc: 62.94%\nEpoch 31/50 | Train Loss: 1.0063 | Val Loss: 1.0494 | Val Acc: 63.36%\nEpoch 32/50 | Train Loss: 1.0073 | Val Loss: 1.0597 | Val Acc: 63.42%\nEpoch 33/50 | Train Loss: 1.0016 | Val Loss: 1.0593 | Val Acc: 63.44%\nEpoch 34/50 | Train Loss: 0.9978 | Val Loss: 1.0576 | Val Acc: 63.76%\nEpoch 35/50 | Train Loss: 0.9960 | Val Loss: 1.0598 | Val Acc: 63.24%\nEpoch 36/50 | Train Loss: 0.9936 | Val Loss: 1.0551 | Val Acc: 63.40%\nEpoch 37/50 | Train Loss: 0.9935 | Val Loss: 1.0553 | Val Acc: 63.04%\nEpoch 38/50 | Train Loss: 0.9921 | Val Loss: 1.0486 | Val Acc: 63.38%\nEpoch 39/50 | Train Loss: 0.9877 | Val Loss: 1.0413 | Val Acc: 63.88%\nEpoch 40/50 | Train Loss: 0.9836 | Val Loss: 1.0538 | Val Acc: 63.44%\nEpoch 41/50 | Train Loss: 0.9827 | Val Loss: 1.0414 | Val Acc: 64.36%\nEpoch 42/50 | Train Loss: 0.9826 | Val Loss: 1.0455 | Val Acc: 64.30%\nEpoch 43/50 | Train Loss: 0.9806 | Val Loss: 1.0627 | Val Acc: 63.82%\nEpoch 44/50 | Train Loss: 0.9775 | Val Loss: 1.0563 | Val Acc: 64.00%\nEpoch 45/50 | Train Loss: 0.9760 | Val Loss: 1.0542 | Val Acc: 63.98%\nEpoch 46/50 | Train Loss: 0.9750 | Val Loss: 1.0514 | Val Acc: 64.38%\nEpoch 47/50 | Train Loss: 0.9739 | Val Loss: 1.0624 | Val Acc: 63.28%\nEpoch 48/50 | Train Loss: 0.9737 | Val Loss: 1.0424 | Val Acc: 64.12%\nEpoch 49/50 | Train Loss: 0.9708 | Val Loss: 1.0591 | Val Acc: 63.82%\nEpoch 50/50 | Train Loss: 0.9674 | Val Loss: 1.0563 | Val Acc: 63.68%\nModel saved to /kaggle/working/models/bayesian_r_layer_seed123.pth\n\n============================================================\nRUNNING SEED 456\n============================================================\nUsing CIFAR-10-C from /kaggle/input/cifar-10/CIFAR-10-C\n\n--- Training layer ---\nEpoch  1/50 | Train Loss: 1.7392 | Val Loss: 1.4389 | Val Acc: 48.34%\nEpoch  2/50 | Train Loss: 1.4448 | Val Loss: 1.3192 | Val Acc: 53.12%\nEpoch  3/50 | Train Loss: 1.3513 | Val Loss: 1.2446 | Val Acc: 55.64%\nEpoch  4/50 | Train Loss: 1.2945 | Val Loss: 1.2042 | Val Acc: 57.74%\nEpoch  5/50 | Train Loss: 1.2532 | Val Loss: 1.2026 | Val Acc: 57.68%\nEpoch  6/50 | Train Loss: 1.2220 | Val Loss: 1.1497 | Val Acc: 59.66%\nEpoch  7/50 | Train Loss: 1.1957 | Val Loss: 1.1382 | Val Acc: 60.10%\nEpoch  8/50 | Train Loss: 1.1777 | Val Loss: 1.1210 | Val Acc: 60.70%\nEpoch  9/50 | Train Loss: 1.1604 | Val Loss: 1.1137 | Val Acc: 61.00%\nEpoch 10/50 | Train Loss: 1.1458 | Val Loss: 1.0916 | Val Acc: 60.90%\nEpoch 11/50 | Train Loss: 1.1326 | Val Loss: 1.0956 | Val Acc: 61.16%\nEpoch 12/50 | Train Loss: 1.1208 | Val Loss: 1.1061 | Val Acc: 60.50%\nEpoch 13/50 | Train Loss: 1.1118 | Val Loss: 1.0842 | Val Acc: 61.74%\nEpoch 14/50 | Train Loss: 1.1023 | Val Loss: 1.0659 | Val Acc: 62.66%\nEpoch 15/50 | Train Loss: 1.0957 | Val Loss: 1.0626 | Val Acc: 62.34%\nEpoch 16/50 | Train Loss: 1.0869 | Val Loss: 1.0868 | Val Acc: 61.58%\nEpoch 17/50 | Train Loss: 1.0818 | Val Loss: 1.0802 | Val Acc: 62.16%\nEpoch 18/50 | Train Loss: 1.0757 | Val Loss: 1.0681 | Val Acc: 62.90%\nEpoch 19/50 | Train Loss: 1.0699 | Val Loss: 1.0547 | Val Acc: 62.88%\nEpoch 20/50 | Train Loss: 1.0636 | Val Loss: 1.0576 | Val Acc: 63.02%\nEpoch 21/50 | Train Loss: 1.0592 | Val Loss: 1.0470 | Val Acc: 63.30%\nEpoch 22/50 | Train Loss: 1.0528 | Val Loss: 1.0483 | Val Acc: 63.60%\nEpoch 23/50 | Train Loss: 1.0498 | Val Loss: 1.0573 | Val Acc: 63.50%\nEpoch 24/50 | Train Loss: 1.0460 | Val Loss: 1.0507 | Val Acc: 63.58%\nEpoch 25/50 | Train Loss: 1.0425 | Val Loss: 1.0580 | Val Acc: 63.18%\nEpoch 26/50 | Train Loss: 1.0374 | Val Loss: 1.0591 | Val Acc: 63.42%\nEpoch 27/50 | Train Loss: 1.0342 | Val Loss: 1.0510 | Val Acc: 63.62%\nEpoch 28/50 | Train Loss: 1.0301 | Val Loss: 1.0409 | Val Acc: 63.72%\nEpoch 29/50 | Train Loss: 1.0272 | Val Loss: 1.0414 | Val Acc: 64.14%\nEpoch 30/50 | Train Loss: 1.0241 | Val Loss: 1.0375 | Val Acc: 63.78%\nEpoch 31/50 | Train Loss: 1.0210 | Val Loss: 1.0508 | Val Acc: 63.92%\nEpoch 32/50 | Train Loss: 1.0185 | Val Loss: 1.0491 | Val Acc: 63.92%\nEpoch 33/50 | Train Loss: 1.0150 | Val Loss: 1.0333 | Val Acc: 63.68%\nEpoch 34/50 | Train Loss: 1.0143 | Val Loss: 1.0416 | Val Acc: 64.06%\nEpoch 35/50 | Train Loss: 1.0112 | Val Loss: 1.0310 | Val Acc: 64.46%\nEpoch 36/50 | Train Loss: 1.0089 | Val Loss: 1.0278 | Val Acc: 64.34%\nEpoch 37/50 | Train Loss: 1.0065 | Val Loss: 1.0421 | Val Acc: 64.06%\nEpoch 38/50 | Train Loss: 1.0050 | Val Loss: 1.0386 | Val Acc: 64.16%\nEpoch 39/50 | Train Loss: 1.0023 | Val Loss: 1.0376 | Val Acc: 64.46%\nEpoch 40/50 | Train Loss: 0.9995 | Val Loss: 1.0359 | Val Acc: 64.70%\nEpoch 41/50 | Train Loss: 0.9982 | Val Loss: 1.0400 | Val Acc: 64.44%\nEpoch 42/50 | Train Loss: 0.9979 | Val Loss: 1.0400 | Val Acc: 64.42%\nEpoch 43/50 | Train Loss: 0.9947 | Val Loss: 1.0584 | Val Acc: 63.80%\nEpoch 44/50 | Train Loss: 0.9916 | Val Loss: 1.0224 | Val Acc: 64.52%\nEpoch 45/50 | Train Loss: 0.9912 | Val Loss: 1.0419 | Val Acc: 64.14%\nEpoch 46/50 | Train Loss: 0.9886 | Val Loss: 1.0381 | Val Acc: 64.22%\nEpoch 47/50 | Train Loss: 0.9900 | Val Loss: 1.0463 | Val Acc: 64.10%\nEpoch 48/50 | Train Loss: 0.9871 | Val Loss: 1.0337 | Val Acc: 64.14%\nEpoch 49/50 | Train Loss: 0.9854 | Val Loss: 1.0385 | Val Acc: 64.08%\nEpoch 50/50 | Train Loss: 0.9824 | Val Loss: 1.0283 | Val Acc: 64.30%\nModel saved to /kaggle/working/models/layer_seed456.pth\n\n--- Training r_layer ---\nEpoch  1/50 | Train Loss: 1.7398 | Val Loss: 1.4668 | Val Acc: 47.98%\nEpoch  2/50 | Train Loss: 1.4317 | Val Loss: 1.2847 | Val Acc: 54.78%\nEpoch  3/50 | Train Loss: 1.3366 | Val Loss: 1.1980 | Val Acc: 58.26%\nEpoch  4/50 | Train Loss: 1.2791 | Val Loss: 1.1582 | Val Acc: 59.02%\nEpoch  5/50 | Train Loss: 1.2378 | Val Loss: 1.1296 | Val Acc: 60.44%\nEpoch  6/50 | Train Loss: 1.2086 | Val Loss: 1.1009 | Val Acc: 60.66%\nEpoch  7/50 | Train Loss: 1.1833 | Val Loss: 1.1044 | Val Acc: 61.18%\nEpoch  8/50 | Train Loss: 1.1633 | Val Loss: 1.0985 | Val Acc: 61.30%\nEpoch  9/50 | Train Loss: 1.1471 | Val Loss: 1.0792 | Val Acc: 61.90%\nEpoch 10/50 | Train Loss: 1.1301 | Val Loss: 1.0658 | Val Acc: 62.24%\nEpoch 11/50 | Train Loss: 1.1188 | Val Loss: 1.0499 | Val Acc: 62.66%\nEpoch 12/50 | Train Loss: 1.1075 | Val Loss: 1.0501 | Val Acc: 62.84%\nEpoch 13/50 | Train Loss: 1.0964 | Val Loss: 1.0486 | Val Acc: 62.84%\nEpoch 14/50 | Train Loss: 1.0892 | Val Loss: 1.0464 | Val Acc: 62.66%\nEpoch 15/50 | Train Loss: 1.0785 | Val Loss: 1.0422 | Val Acc: 63.36%\nEpoch 16/50 | Train Loss: 1.0730 | Val Loss: 1.0300 | Val Acc: 64.22%\nEpoch 17/50 | Train Loss: 1.0637 | Val Loss: 1.0265 | Val Acc: 64.40%\nEpoch 18/50 | Train Loss: 1.0571 | Val Loss: 1.0282 | Val Acc: 63.98%\nEpoch 19/50 | Train Loss: 1.0513 | Val Loss: 1.0211 | Val Acc: 64.20%\nEpoch 20/50 | Train Loss: 1.0458 | Val Loss: 1.0258 | Val Acc: 64.26%\nEpoch 21/50 | Train Loss: 1.0414 | Val Loss: 1.0116 | Val Acc: 65.26%\nEpoch 22/50 | Train Loss: 1.0373 | Val Loss: 1.0264 | Val Acc: 64.66%\nEpoch 23/50 | Train Loss: 1.0331 | Val Loss: 1.0270 | Val Acc: 64.90%\nEpoch 24/50 | Train Loss: 1.0269 | Val Loss: 1.0359 | Val Acc: 64.22%\nEpoch 25/50 | Train Loss: 1.0231 | Val Loss: 1.0353 | Val Acc: 64.30%\nEpoch 26/50 | Train Loss: 1.0213 | Val Loss: 1.0182 | Val Acc: 65.28%\nEpoch 27/50 | Train Loss: 1.0177 | Val Loss: 1.0156 | Val Acc: 64.84%\nEpoch 28/50 | Train Loss: 1.0146 | Val Loss: 1.0039 | Val Acc: 65.40%\nEpoch 29/50 | Train Loss: 1.0086 | Val Loss: 1.0160 | Val Acc: 65.32%\nEpoch 30/50 | Train Loss: 1.0078 | Val Loss: 1.0103 | Val Acc: 64.82%\nEpoch 31/50 | Train Loss: 1.0052 | Val Loss: 0.9912 | Val Acc: 65.50%\nEpoch 32/50 | Train Loss: 0.9994 | Val Loss: 1.0221 | Val Acc: 64.68%\nEpoch 33/50 | Train Loss: 1.0001 | Val Loss: 1.0006 | Val Acc: 65.16%\nEpoch 34/50 | Train Loss: 0.9943 | Val Loss: 0.9992 | Val Acc: 65.08%\nEpoch 35/50 | Train Loss: 0.9904 | Val Loss: 1.0013 | Val Acc: 65.34%\nEpoch 36/50 | Train Loss: 0.9918 | Val Loss: 0.9932 | Val Acc: 65.20%\nEpoch 37/50 | Train Loss: 0.9892 | Val Loss: 0.9989 | Val Acc: 65.14%\nEpoch 38/50 | Train Loss: 0.9848 | Val Loss: 1.0095 | Val Acc: 65.50%\nEpoch 39/50 | Train Loss: 0.9834 | Val Loss: 0.9966 | Val Acc: 65.46%\nEpoch 40/50 | Train Loss: 0.9837 | Val Loss: 0.9941 | Val Acc: 65.36%\nEpoch 41/50 | Train Loss: 0.9812 | Val Loss: 0.9980 | Val Acc: 65.78%\nEpoch 42/50 | Train Loss: 0.9782 | Val Loss: 0.9963 | Val Acc: 65.98%\nEpoch 43/50 | Train Loss: 0.9743 | Val Loss: 1.0039 | Val Acc: 65.44%\nEpoch 44/50 | Train Loss: 0.9748 | Val Loss: 0.9994 | Val Acc: 65.90%\nEpoch 45/50 | Train Loss: 0.9701 | Val Loss: 1.0049 | Val Acc: 65.40%\nEpoch 46/50 | Train Loss: 0.9693 | Val Loss: 1.0070 | Val Acc: 65.96%\nEpoch 47/50 | Train Loss: 0.9671 | Val Loss: 1.0036 | Val Acc: 65.70%\nEpoch 48/50 | Train Loss: 0.9666 | Val Loss: 0.9865 | Val Acc: 66.88%\nEpoch 49/50 | Train Loss: 0.9649 | Val Loss: 0.9882 | Val Acc: 66.72%\nEpoch 50/50 | Train Loss: 0.9635 | Val Loss: 0.9929 | Val Acc: 66.24%\nModel saved to /kaggle/working/models/r_layer_seed456.pth\n\n--- Training bayesian_r_layer ---\nEpoch  1/50 | Train Loss: 1.7308 | Val Loss: 1.4272 | Val Acc: 49.66%\nEpoch  2/50 | Train Loss: 1.4275 | Val Loss: 1.2980 | Val Acc: 54.32%\nEpoch  3/50 | Train Loss: 1.3300 | Val Loss: 1.2247 | Val Acc: 57.26%\nEpoch  4/50 | Train Loss: 1.2769 | Val Loss: 1.1650 | Val Acc: 58.68%\nEpoch  5/50 | Train Loss: 1.2382 | Val Loss: 1.1383 | Val Acc: 59.86%\nEpoch  6/50 | Train Loss: 1.2095 | Val Loss: 1.1307 | Val Acc: 60.22%\nEpoch  7/50 | Train Loss: 1.1851 | Val Loss: 1.1023 | Val Acc: 61.52%\nEpoch  8/50 | Train Loss: 1.1660 | Val Loss: 1.0893 | Val Acc: 61.88%\nEpoch  9/50 | Train Loss: 1.1493 | Val Loss: 1.0789 | Val Acc: 61.94%\nEpoch 10/50 | Train Loss: 1.1372 | Val Loss: 1.0804 | Val Acc: 61.98%\nEpoch 11/50 | Train Loss: 1.1242 | Val Loss: 1.0775 | Val Acc: 62.40%\nEpoch 12/50 | Train Loss: 1.1124 | Val Loss: 1.0493 | Val Acc: 62.40%\nEpoch 13/50 | Train Loss: 1.1036 | Val Loss: 1.0443 | Val Acc: 62.86%\nEpoch 14/50 | Train Loss: 1.0952 | Val Loss: 1.0305 | Val Acc: 63.42%\nEpoch 15/50 | Train Loss: 1.0869 | Val Loss: 1.0250 | Val Acc: 63.66%\nEpoch 16/50 | Train Loss: 1.0786 | Val Loss: 1.0389 | Val Acc: 63.64%\nEpoch 17/50 | Train Loss: 1.0713 | Val Loss: 1.0185 | Val Acc: 64.34%\nEpoch 18/50 | Train Loss: 1.0647 | Val Loss: 1.0168 | Val Acc: 64.28%\nEpoch 19/50 | Train Loss: 1.0581 | Val Loss: 1.0389 | Val Acc: 63.26%\nEpoch 20/50 | Train Loss: 1.0558 | Val Loss: 1.0169 | Val Acc: 64.68%\nEpoch 21/50 | Train Loss: 1.0460 | Val Loss: 1.0245 | Val Acc: 64.22%\nEpoch 22/50 | Train Loss: 1.0450 | Val Loss: 1.0358 | Val Acc: 63.58%\nEpoch 23/50 | Train Loss: 1.0383 | Val Loss: 1.0337 | Val Acc: 64.48%\nEpoch 24/50 | Train Loss: 1.0350 | Val Loss: 1.0053 | Val Acc: 64.90%\nEpoch 25/50 | Train Loss: 1.0311 | Val Loss: 1.0114 | Val Acc: 65.10%\nEpoch 26/50 | Train Loss: 1.0258 | Val Loss: 1.0133 | Val Acc: 64.58%\nEpoch 27/50 | Train Loss: 1.0213 | Val Loss: 1.0178 | Val Acc: 65.00%\nEpoch 28/50 | Train Loss: 1.0187 | Val Loss: 1.0225 | Val Acc: 64.36%\nEpoch 29/50 | Train Loss: 1.0164 | Val Loss: 1.0137 | Val Acc: 64.92%\nEpoch 30/50 | Train Loss: 1.0122 | Val Loss: 1.0150 | Val Acc: 64.80%\nEpoch 31/50 | Train Loss: 1.0105 | Val Loss: 1.0176 | Val Acc: 64.76%\nEpoch 32/50 | Train Loss: 1.0077 | Val Loss: 1.0164 | Val Acc: 65.04%\nEpoch 33/50 | Train Loss: 1.0046 | Val Loss: 1.0118 | Val Acc: 64.82%\nEpoch 34/50 | Train Loss: 1.0005 | Val Loss: 1.0023 | Val Acc: 65.02%\nEpoch 35/50 | Train Loss: 0.9979 | Val Loss: 1.0068 | Val Acc: 65.46%\nEpoch 36/50 | Train Loss: 0.9949 | Val Loss: 1.0052 | Val Acc: 65.12%\nEpoch 37/50 | Train Loss: 0.9952 | Val Loss: 1.0233 | Val Acc: 64.28%\nEpoch 38/50 | Train Loss: 0.9938 | Val Loss: 1.0056 | Val Acc: 65.34%\nEpoch 39/50 | Train Loss: 0.9881 | Val Loss: 1.0167 | Val Acc: 64.60%\nEpoch 40/50 | Train Loss: 0.9863 | Val Loss: 1.0183 | Val Acc: 65.12%\nEpoch 41/50 | Train Loss: 0.9846 | Val Loss: 0.9914 | Val Acc: 65.90%\nEpoch 42/50 | Train Loss: 0.9829 | Val Loss: 0.9991 | Val Acc: 65.24%\nEpoch 43/50 | Train Loss: 0.9792 | Val Loss: 1.0023 | Val Acc: 65.12%\nEpoch 44/50 | Train Loss: 0.9790 | Val Loss: 0.9907 | Val Acc: 65.92%\nEpoch 45/50 | Train Loss: 0.9781 | Val Loss: 0.9954 | Val Acc: 66.16%\nEpoch 46/50 | Train Loss: 0.9749 | Val Loss: 1.0154 | Val Acc: 65.10%\nEpoch 47/50 | Train Loss: 0.9726 | Val Loss: 0.9877 | Val Acc: 65.74%\nEpoch 48/50 | Train Loss: 0.9721 | Val Loss: 0.9951 | Val Acc: 66.10%\nEpoch 49/50 | Train Loss: 0.9719 | Val Loss: 0.9911 | Val Acc: 66.22%\nEpoch 50/50 | Train Loss: 0.9681 | Val Loss: 0.9965 | Val Acc: 65.66%\nModel saved to /kaggle/working/models/bayesian_r_layer_seed456.pth\nResults saved to /kaggle/working/results_summary.csv\n\n✅ All plots saved in /kaggle/working/visualizations\n\n============================================================\nEXPERIMENT COMPLETE. Check /kaggle/working/ for outputs.\n============================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}